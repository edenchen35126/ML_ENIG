import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import OrdinalEncoder, StandardScaler,OneHotEncoder, FunctionTransformer
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import KFold, RandomizedSearchCV,train_test_split
from collections import defaultdict
from scipy.stats import randint, uniform
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
import shap
from joblib import Memory
from xgboost import XGBRegressor
from xgboost import XGBRegressor, callback as xcb
from xgboost.callback import EarlyStopping
from sklearn.tree import DecisionTreeRegressor
from lightgbm import LGBMRegressor
from sklearn import set_config
set_config(transform_output="pandas")  # âœ… è®“ ColumnTransformer è‡ªå‹•è¼¸å‡º DataFrame
from sklearn.base import BaseEstimator, TransformerMixin
import time
import polars as pl

# =========================
# è¦–è¦ºåŒ–å­—å‹ï¼ˆå¯ç•™å¯å»ï¼‰
# =========================
plt.rcParams["font.sans-serif"] = ["Microsoft JhengHei"]
plt.rcParams["axes.unicode_minus"] = False

# =========================
# å…¨åŸŸè¨­å®š
# =========================
RANDOM_STATE = 42
OUTER_FOLDS = 5        # å¤–å±¤ KFoldï¼ˆæœ€çµ‚è©•ä¼°ï¼‰
INNER_FOLDS = 5        # å…§å±¤ KFoldï¼ˆåƒæ•¸æœå°‹ç”¨ï¼‰
N_ITER = 30            # RandomizedSearchCV æŠ½æ¨£æ¬¡æ•¸ï¼ˆå¯è¦–ç®—åŠ›èª¿æ•´ï¼‰  å¾åƒæ•¸åˆ†ä½ˆ (param_distributions) ä¸­éš¨æ©ŸæŠ½å– 60 çµ„ä¸åŒçš„åƒæ•¸çµ„åˆä¾†è¨“ç·´èˆ‡äº¤å‰é©—è­‰ã€‚

# =========================
# è®€æª”
# =========================
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
data_path = os.path.join(BASE_DIR, "data", "raw", "åŒ–é‡‘ç·šè‡ªä¸»æª¢æŸ¥è¡¨_all.csv")
df = pd.read_csv(data_path, encoding="big5")

df = pd.read_csv(data_path, encoding="big5")

print("=== æ¬„ä½ç¼ºå€¼æ•¸é‡ ===")
print(df.isnull().sum())


# =========================
# ç›®æ¨™èˆ‡ç‰¹å¾µå®£å‘Šï¼ˆåŸå§‹æ¬„ä½åï¼‰
# =========================
label_col = "é‡‘"

# features = ['æ­¸å±¬ç­åˆ¥','æ–™è™Ÿ','çŸ­æ‰¹','å­æ‰¹','æ‰¹è™Ÿ','æ•¸é‡','MTO1','MTO2','æª¢æŸ¥å‹æ…‹','é …ç›®','é³','é‡‘åšä¸‹é™','é‡‘åšä¸Šé™','é³åšä¸‹é™','é³åšä¸Šé™','æ¿å­é¡å‹','é›»æµå€¼1','é›»æµå€¼2',
#             'æ§½æ¬¡1','æ§½æ¬¡2','ç·šåˆ¥']
features = ['é‡‘åšä¸‹é™','æ¿å­é¡å‹','æ­¸å±¬ç­åˆ¥','é‡‘åšä¸Šé™','ç·šåˆ¥','æ§½æ¬¡1','é³','æ•¸é‡','æª¢æŸ¥å‹æ…‹','é³åšä¸‹é™','MTO1','é³åšä¸Šé™','é›»æµå€¼1','é›»æµå€¼2','MTO2','é …ç›®']
#features = ['é›»æµå€¼1','é›»æµå€¼2','MTO1','MTO2','å­æ‰¹','æ–™è™Ÿ','çŸ­æ‰¹','æ‰¹è™Ÿ']


# =========================
# XGBOOST åƒæ•¸æœå°‹ç©ºé–“
# =========================
# def get_param_dist_xgb():
#     """XGBoost é«˜ç¶­åº¦è³‡æ–™é©ç”¨çš„ RandomizedSearch åƒæ•¸ç©ºé–“ï¼ˆå·²ç§»é™¤ 'auto'ï¼‰"""
#     return {
#         # === æ¨¹çš„çµæ§‹åƒæ•¸ï¼ˆåä¿å®ˆè¨­å®šï¼Œé¿å…é«˜ç¶­ç‰¹å¾µéæ“¬åˆï¼‰ ===
#         "model__n_estimators": np.linspace(300, 900, 7, dtype=int),  # æ¨¹æ•¸é‡
#         "model__max_depth": [3, 5, 7, 9, 12],                       # é¿å…å¤ªæ·±å°è‡´é«˜ç¶­éæ“¬åˆ
#         "model__min_child_weight": [1, 3, 5, 7, 10],                 # ç¯€é»æœ€å°æ¬Šé‡å’Œ
        
#         # === å­¸ç¿’ç‡èˆ‡æ”¶æ–‚æ§åˆ¶ï¼ˆlearning rate è¼ƒä½ä»¥æå‡ç©©å®šæ€§ï¼‰ ===
#         "model__learning_rate": [0.01, 0.05, 0.1, 0.15, 0.2],        # Boosting æ­¥é•·ï¼Œ0.05~0.1 è¼ƒç©©å®š
#         "model__gamma": [0, 0.5, 1, 2, 5],                           # åˆ†è£‚æ‡²ç½°é …ï¼Œå¢åŠ å¯æ³›åŒ–æ€§
        
#         # === å­æ¨£æœ¬èˆ‡ç‰¹å¾µå–æ¨£ï¼ˆé«˜ç¶­ç‰¹å¾µéœ€æ›´é«˜éš¨æ©Ÿæ€§ä»¥æ§åˆ¶æ–¹å·®ï¼‰ ===
#         "model__subsample": [0.6, 0.7, 0.8, 0.9, 1.0],               # è¡Œå–æ¨£æ¯”ä¾‹ï¼ˆæ¨£æœ¬å±¤ç´šï¼‰
#         "model__colsample_bytree": [0.3, 0.4, 0.5, 0.6, 0.7],        # åˆ—å–æ¨£æ¯”ä¾‹ï¼ˆç‰¹å¾µå±¤ç´šï¼Œç‰¹åˆ¥é‡è¦ï¼‰
        
#         # === æ­£å‰‡åŒ–æ§åˆ¶ï¼ˆé¿å…é«˜ç¶­æ¬Šé‡çˆ†ç‚¸ï¼‰ ===
#         "model__reg_lambda": [0.5, 1.0, 1.5, 2.0, 3.0],              # L2 æ­£å‰‡åŒ–
#         "model__reg_alpha": [0, 0.1, 0.3, 0.5, 1.0],                 # L1 æ­£å‰‡åŒ–

#         # === æ¨¡å‹çµæ§‹ ===
#         "model__booster": ["gbtree", "dart"],                        # dart æ”¯æ´ dropoutï¼Œé™ä½éæ“¬åˆ
#         "model__tree_method": ["hist"],                              # é©åˆä¸­å¤§å‹è³‡æ–™é›†
#     }

def parse_dates(df: pd.DataFrame, date_cols):
    out = df.copy()
    for c in date_cols:
        try:
            out[c] = pd.to_datetime(out[c], errors="coerce")
        except Exception:
            out[c] = pd.NaT
    return out


def add_date_features(df: pd.DataFrame, date_col: str):
    out = df.copy()
    col = date_col
    out[f"{col}_year"] = out[col].dt.year
    out[f"{col}_month"] = out[col].dt.month
    out[f"{col}_day"] = out[col].dt.day
    out[f"{col}_dow"] = out[col].dt.dayofweek
    out[f"{col}_hour"] = out[col].dt.hour
    return out

# å¯åˆ—å‡ºè¼¸å‡ºç‰¹å¾µåç¨±çš„é€±æœŸè½‰æ›å™¨
class CyclicalEncoder(BaseEstimator, TransformerMixin):
    """å¯åˆ—å‡ºè¼¸å‡ºç‰¹å¾µåç¨±çš„é€±æœŸè½‰æ›å™¨"""
    def fit(self, X, y=None):
        self.columns_ = X.columns if isinstance(X, pd.DataFrame) else [f"col_{i}" for i in range(X.shape[1])]
        return self

    def transform(self, X):
        if isinstance(X, np.ndarray):
            X = pd.DataFrame(X, columns=self.columns_)
        return cyclical_encode(X)

    def get_feature_names_out(self, input_features=None):
        # ç”¨ç©ºè³‡æ–™æ¡†è·‘ä¸€é cyclical_encode() å–å¾—æ–°æ¬„ä½å
        if input_features is None:
            input_features = getattr(self, "columns_", [])
        df_temp = pd.DataFrame({c: [0] for c in input_features})
        df_encoded = cyclical_encode(df_temp)
        return df_encoded.columns.to_numpy()

def cyclical_encode(df):
    df = df.copy()

    for c in df.columns:
        if c.endswith("_hour"):
            df[f"{c}_sin"] = np.sin(2 * np.pi * df[c] / 24)
            df[f"{c}_cos"] = np.cos(2 * np.pi * df[c] / 24)
            # time_features.append(f"{c}_sin")
            # time_features.append(f"{c}_cos")
            #out_cols += [f"{c}_sin", f"{c}_cos"]
        elif c.endswith("_dow"):
            df[f"{c}_sin"] = np.sin(2 * np.pi * df[c] / 7)
            df[f"{c}_cos"] = np.cos(2 * np.pi * df[c] / 7)
            #out_cols += [f"{c}_sin", f"{c}_cos"]
            # time_features.append(f"{c}_sin")
            # time_features.append(f"{c}_cos")
        elif c.endswith("_month"):
            df[f"{c}_sin"] = np.sin(2 * np.pi * df[c] / 12)
            df[f"{c}_cos"] = np.cos(2 * np.pi * df[c] / 12)
            #out_cols += [f"{c}_sin", f"{c}_cos"]
            # time_features.append(f"{c}_sin")
            # time_features.append(f"{c}_cos")

    return df

# def cyclical_encode(df,time_features):
#     df = df.copy()
#     #out_cols = []
#     for c in df.columns:
#         if c.endswith("_hour"):
#             df[f"{c}_sin"] = np.sin(2 * np.pi * df[c] / 24)
#             df[f"{c}_cos"] = np.cos(2 * np.pi * df[c] / 24)
#             time_features.append(f"{c}_sin")
#             time_features.append(f"{c}_cos")
#             #out_cols += [f"{c}_sin", f"{c}_cos"]
#         elif c.endswith("_dow"):
#             df[f"{c}_sin"] = np.sin(2 * np.pi * df[c] / 7)
#             df[f"{c}_cos"] = np.cos(2 * np.pi * df[c] / 7)
#             #out_cols += [f"{c}_sin", f"{c}_cos"]
#             time_features.append(f"{c}_sin")
#             time_features.append(f"{c}_cos")
#         elif c.endswith("_month"):
#             df[f"{c}_sin"] = np.sin(2 * np.pi * df[c] / 12)
#             df[f"{c}_cos"] = np.cos(2 * np.pi * df[c] / 12)
#             #out_cols += [f"{c}_sin", f"{c}_cos"]
#             time_features.append(f"{c}_sin")
#             time_features.append(f"{c}_cos")
#     return df


# ==== LightGBM åƒæ•¸æœå°‹ç©ºé–“ ====
def get_param_dist_lgb():
    """LightGBM çš„ RandomizedSearch åƒæ•¸ç©ºé–“"""
    return {
        "model__n_estimators": [200, 400, 600, 800, 1000],
        "model__learning_rate": [0.01, 0.05, 0.1, 0.2],
        "model__num_leaves": [15, 31, 63, 127],
        "model__max_depth": [-1, 5, 10, 15],
        "model__min_child_samples": [5, 10, 20, 40],
        "model__subsample": [0.6, 0.8, 1.0],
        "model__colsample_bytree": [0.6, 0.8, 1.0],
        "model__reg_lambda": [0, 0.1, 1, 5, 10],   # L2 regularization
        "model__reg_alpha": [0, 0.1, 0.5, 1],      # L1 regularization
        "model__boosting_type": ["gbdt", "dart"],  # dart æ”¯æ´ dropout boosting
    }


def get_param_dist_dt():
    """Decision Tree å°ˆç”¨çš„ RandomizedSearchCV æœå°‹ç©ºé–“"""
    return {
        "model__max_depth": [3, 5, 7, 10, 15, 20, None],  # æ¨¹çš„æœ€å¤§æ·±åº¦
        "model__min_samples_split": [2, 5, 10, 20],       # åˆ†è£‚æ‰€éœ€çš„æœ€å°æ¨£æœ¬æ•¸
        "model__min_samples_leaf": [1, 2, 4, 8, 10],      # è‘‰ç¯€é»æœ€å°æ¨£æœ¬æ•¸
        "model__max_features": ["sqrt", "log2", None],    # åˆ†è£‚æ™‚è€ƒæ…®çš„ç‰¹å¾µæ•¸
        "model__criterion": ["squared_error", "friedman_mse"],  # æå¤±å‡½æ•¸
        "model__splitter": ["best", "random"]             # åˆ†è£‚ç­–ç•¥
    }

def get_param_dist_xgb():
    # æ”¶æ–‚å¾Œçš„ã€Œè³‡æºå‹å–„ç‰ˆã€æœå°‹ç©ºé–“
    return {
        # "model__n_estimators": np.linspace(200, 500, 7, dtype=int),  # å°ä¸€é»ï¼Œäº¤çµ¦ early stopping
        # "model__max_depth": [3, 5, 7],                               # é™åˆ¶æ¨¹æ·±
        # "model__min_child_weight": [3, 5, 7, 10],                    # å¢åŠ ç¯€é»æœ€å°æ¬Šé‡ï¼ŒæŠ‘åˆ¶éæ“¬åˆ/è¨ˆç®—é‡
        # "model__learning_rate": [0.03, 0.05, 0.07, 0.1],
        # "model__gamma": [0, 0.5, 1, 2],
        # "model__subsample": [0.6, 0.7, 0.8],
        # "model__colsample_bytree": [0.3, 0.4, 0.5],
        # "model__reg_lambda": [0.8, 1.0, 1.5, 2.0],
        # "model__reg_alpha": [0, 0.1, 0.3, 0.5],
        # "model__booster": ["gbtree", "dart"],
        # "model__tree_method": ["hist"],  # æœ‰ GPU å†æ”¹ "gpu_hist"
        # # "model__max_bin": [128, 256]   # å¦‚è¨˜æ†¶é«”åƒç·Šå¯æ‰“é–‹ï¼ˆå° hist æœ‰æ•ˆï¼‰

        "model__n_estimators": randint(1495, 1500),  # å°ä¸€é»ï¼Œäº¤çµ¦ early stopping
        #"model__n_estimators": [200, 300, 400, 500],
        "model__max_depth": [3, 5, 7],                               # é™åˆ¶æ¨¹æ·±
        "model__min_child_weight": [3, 5, 7, 10],                    # å¢åŠ ç¯€é»æœ€å°æ¬Šé‡ï¼ŒæŠ‘åˆ¶éæ“¬åˆ/è¨ˆç®—é‡
        "model__learning_rate": [0.03, 0.05, 0.07, 0.1],
        "model__gamma": [0, 0.5, 1, 2],
        "model__subsample": [0.6, 0.7, 0.8],
        "model__colsample_bytree": [0.3, 0.4, 0.5],
        "model__reg_lambda": [0.8, 1.0, 1.5, 2.0],
        "model__reg_alpha": [0, 0.1, 0.3, 0.5],
        "model__booster": ["gbtree"],
        "model__tree_method": ["hist"],  # æœ‰ GPU å†æ”¹ "gpu_hist"
        # "model__max_bin": [128, 256]   # å¦‚è¨˜æ†¶é«”åƒç·Šå¯æ‰“é–‹ï¼ˆå° hist æœ‰æ•ˆï¼‰
    }


# =========================
# RF åƒæ•¸æœå°‹ç©ºé–“
# =========================
def get_param_dist():
    """RandomizedSearch çš„åƒæ•¸ç©ºé–“ï¼ˆå·²ç§»é™¤ 'auto'ï¼Œé¿å…æ–°ç‰ˆ sklearn éŒ¯èª¤ï¼‰"""
    return {
        "model__n_estimators": np.linspace(200, 800, 7, dtype=int),
        "model__max_depth": [None, 10, 15, 20, 25, 30],
        "model__min_samples_split": [2, 5, 10, 20],
        "model__min_samples_leaf": [1, 2, 4, 8],
        "model__max_features": ["sqrt", "log2", 0.3, 0.5, 0.7],  #é«˜ç¶­åº¦è¼ƒé©åˆ (æœç´¢ç©ºé–“æ”¶æ–‚ã€æ–¹å·®è¼ƒå°)
        "model__bootstrap": [True, False],
    }
def get_rf_param_distributions(): #ä¸é©åˆé«˜ç¶­åº¦è³‡æ–™é‡è·Ÿç¶­åº¦
    return {
        "model__n_estimators": randint(300, 1201),     # 300~1200
        "model__max_depth":   [None] + list(range(5, 51)),
        # è‹¥å¸Œæœ›ç”¨ã€Œæ•´æ•¸ç‰¹å¾µæ•¸ã€è€Œéæ¯”ä¾‹ï¼Œæ”¹æˆï¼šrandint(2, 1 + len(features))
        "model__max_features": uniform(0.3, 0.6),      # 0.3~0.9 çš„æ¯”ä¾‹    #ä¸é©åˆé«˜ç¶­åº¦æ•¸æ“šï¼Œå› ç‚ºæœƒè®“ RF åœ¨æ¯å€‹ç¯€é»çœ‹åˆ°å¤ªå¤šç‰¹å¾µï¼Œåœ¨æ¨£æœ¬é‡æœ‰é™ã€è¨Šè™Ÿç¨€ç–æ™‚æœƒä½¿æ¨¡å‹ä¸ç©©
        "model__min_samples_split": randint(2, 21),    # 2~20
        "model__min_samples_leaf":  randint(1, 11),    # 1~10
        "model__bootstrap": [True, False],
    }

def ramdom_forest_model(x_train,y_train):
    rf = RandomForestRegressor(
        random_state=42,
        n_estimators=600,
        oob_score=False,
        min_samples_split = 10,
        min_samples_leaf = 1,
        max_features = 7,
        max_depth = None,
        bootstrap = True
    )

    rf.fit(x_train, y_train)

    return rf

def iqr_clip(series, q1=0.25, q3=0.75, k=3.0):
    """Robustly clip outliers; default k=3 (æ¯” 1.5 å¯¬é¬†ã€è¼ƒç©©å®š)"""
    s = pd.Series(series)
    q_low, q_hi = s.quantile(q1), s.quantile(q3)
    iqr = q_hi - q_low
    lo = q_low - k * iqr
    hi = q_hi + k * iqr
    return s.clip(lower=lo, upper=hi)

def Std(df_num):
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(df_num)

    
    X_scaled_df = pd.DataFrame(
        X_scaled,
        columns=num_cols,
        index=df_num.index
    )

    return X_scaled_df

def one_hot(df_cat):
    encoder = OneHotEncoder(handle_unknown="ignore", sparse_output=False)
    X_encoded = encoder.fit_transform(df_cat)

    X_encoded_df = pd.DataFrame(
        X_encoded,
        columns=encoder.get_feature_names_out(cat_cols),  # é€™è£¡ä¸€å®šè¦ç”¨ encoderï¼Œè€Œä¸æ˜¯ function
        index=df_cat.index
    )

    return X_encoded_df

# ==== [Preprocess & Model Builders] ==========================================
def build_preprocessor(df: pd.DataFrame, features: list, time_features: list):
    """å»ºæ§‹ ColumnTransformer å‰è™•ç†ç®¡ç·š"""
    cat_cols = [c for c in features if df[c].dtype == "object"]
    num_cols = [c for c in features if c not in cat_cols and c not in time_features]
    # num_cols = [
    #     c for c in features
    #     if c not in cat_cols
    #     and c not in time_features
    #     and not np.issubdtype(df[c].dtype, np.datetime64)
    # ]

    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler", StandardScaler())
    ])
    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore", sparse_output=False))
    ])

    #time_transformer = FunctionTransformer(cyclical_encode, validate=False)
    

    # ğŸ‘‰ æ–°å¢ä¸€å€‹ã€Œtime passthroughã€åˆ†æ”¯
    #passthrough_transformer = "passthrough"

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, num_cols),
            ("cat", categorical_transformer, cat_cols),
            #("time", passthrough_transformer, time_features),  # âœ… ç›´æ¥å¸¶å…¥ä¸è®Šå‹•
            #("time", time_transformer, time_features),  # åˆ†æ”¯ï¼šæ™‚é–“ç‰¹å¾µè½‰é€±æœŸç·¨ç¢¼
            ("time", CyclicalEncoder(), time_features)
        ]
    )
    return preprocessor, cat_cols, num_cols

# =========================
# å¤–å±¤ KFoldï¼ˆNested CVï¼‰
# =========================
outer_kf = KFold(n_splits=OUTER_FOLDS, shuffle=True, random_state=RANDOM_STATE)

fold_metrics = []
fi_collector = defaultdict(list)  # è·¨æŠ˜ç‰¹å¾µé‡è¦åº¦å½™æ•´


# ---- Date features ----
used_time_col = None

used_time_col = "ç”Ÿç”¢æ—¥æœŸ"

if used_time_col:
    df = parse_dates(df, [used_time_col])
    df = add_date_features(df, used_time_col)
    #time_features = []
    time_features = [
        f"{used_time_col}_year",
        f"{used_time_col}_month",
        f"{used_time_col}_day",
        f"{used_time_col}_dow", # day of week
        f"{used_time_col}_hour"
    ]
    features.extend(time_features)
else:
    time_features = []

#df = cyclical_encode(df,time_features)  # é€™è£¡ç›´æ¥å‘¼å«ä½ çš„å‡½å¼


for fold_idx, (train_idx, test_idx) in enumerate(outer_kf.split(df), start=1):
    # cat_cols = [c for c in features if df[c].dtype == "object"]
    # num_cols = [c for c in features if c not in cat_cols]

    # # ---- Numeric cleanup ----
    # for c in num_cols:
    #     try:
    #         df[c] = pd.to_numeric(df[c], errors="coerce")
    #         df[c] = iqr_clip(df[c])
    #     except Exception:
    #         pass

    # target = "é‡‘"
    # X = df[features].copy()
    # y = df[target].astype(float)
    # # ---- Train/Test split ----
    # X_train, X_test, y_train, y_test = train_test_split(
    #     X, y, test_size=0.2, random_state=42
    # )

    df_train = df.iloc[train_idx].copy()
    df_test  = df.iloc[test_idx].copy()

    
    cat_cols = [c for c in features if df_train[c].dtype == "object"]
    num_cols = [c for c in features if c not in cat_cols]


    # used_time_col = "ç”Ÿç”¢æ—¥æœŸ"

    # if used_time_col:
    #     df_train = parse_dates(df_train, [used_time_col])
    #     df_train = add_date_features(df_train, used_time_col)
    #     #time_features = []
    #     time_features = [
    #         f"{used_time_col}_year",
    #         f"{used_time_col}_month",
    #         f"{used_time_col}_day",
    #         f"{used_time_col}_dow", # day of week
    #         f"{used_time_col}_hour"
    #     ]
    #     #features.extend(time_features)
    # else:
    #     time_features = []
    
    # df_train = cyclical_encode(df_train,time_features)  # é€™è£¡ç›´æ¥å‘¼å«ä½ çš„å‡½å¼

    # if used_time_col:
    #     df_test = parse_dates(df_test, [used_time_col])
    #     df_test = add_date_features(df_test, used_time_col)
    # else:
    #     time_features = []

    # df_test = cyclical_encode_v2(df_test,time_features)  # é€™è£¡ç›´æ¥å‘¼å«ä½ çš„å‡½å¼




    # if num_cols:
    #     df_num = df[num_cols].fillna(df_train[num_cols].median())
    #     X_scaled_df = Std(df_num)
    # else:
    #     X_scaled_df = pd.DataFrame()

    # if cat_cols:
    #     # 1) å…ˆç®—å‡ºæ¯å€‹é¡åˆ¥æ¬„ä½çš„çœ¾æ•¸ï¼ˆç¬¬ä¸€åˆ—ï¼‰
    #     cat_modes = df_train[cat_cols].mode(dropna=True).iloc[0]
    #     # 2) å°ç›®å‰è¦è™•ç†çš„ dfï¼ˆå¯èƒ½æ˜¯ train æˆ– testï¼‰åšç¼ºå¤±å€¼è£œçœ¾æ•¸
    #     df_cat = df[cat_cols].fillna(cat_modes)
    #     X_encoded_df  = one_hot(df_cat)
    # else:
    #     X_encoded_df = pd.DataFrame()


    # # 3) åˆä½µæˆä¸€å€‹ DataFrame
    # X_final = pd.concat([X_scaled_df, X_encoded_df,df[label_col]], axis=1)

    # df_train = X_final.iloc[train_idx].copy()
    # df_test  = X_final.iloc[test_idx].copy()




    # # 1) ç¼ºå€¼è£œå€¼ï¼ˆåƒ…ç”¨è¨“ç·´ fold çš„çµ±è¨ˆå€¼ï¼‰
    # for col in impute_cols_with_mode:
    #     if col in df_train.columns:
    #         mode_val = df_train[col].mode(dropna=True)
    #         if not mode_val.empty:
    #             mode_val = mode_val[0]  #å¦‚æœæœ‰å¤šå€‹æ•¸å€¼æ˜¯çœ¾æ•¸ï¼Œé¸æ“‡ç¬¬ä¸€å€‹
    #             df_train[col] = df_train[col].fillna(mode_val)
    #             df_test[col]  = df_test[col].fillna(mode_val)

    # # 2) é¡åˆ¥æ¬„ä½ Ordinal ç·¨ç¢¼ï¼ˆåƒ…ç”¨è¨“ç·´ fold fitï¼Œé¿å…æ´©æ¼ï¼‰
    # enc = OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1, dtype=np.int64)
    # enc.fit(df_train[categorical_cols_for_ordinal].astype(str))

    # # ä¸€æ¬¡æ€§ transform è¨“ç·´/æ¸¬è©¦é›†çš„æ‰€æœ‰é¡åˆ¥æ¬„ä½
    # train_enc = enc.transform(df_train[categorical_cols_for_ordinal].astype(str))
    # test_enc  = enc.transform(df_test[categorical_cols_for_ordinal].astype(str))

    # # å°‡å„æ¬„çµæœå›å¡«æˆ *_Code æ¬„ä½ï¼ˆä¿æŒèˆ‡ categorical_cols_for_ordinal çš„é †åºä¸€è‡´ï¼‰
    # for i, c in enumerate(categorical_cols_for_ordinal):
    #     df_train[f"{c}_Code"] = train_enc[:, i].astype(np.int64)
    #     df_test[f"{c}_Code"]  = test_enc[:, i].astype(np.int64)

    # # 3) æ–™è™Ÿé »ç‡ç·¨ç¢¼ï¼ˆåƒ…ç”¨è¨“ç·´ fold çš„åˆ†ä½ˆï¼‰
    # features = []
    # for f in base_raw_features:
    #     if f in categorical_cols_for_ordinal:
    #         features.append(f"{f}_Code")  # ç”¨ç·¨ç¢¼å¾Œæ¬„ä½
    #     else:
    #         features.append(f)

    # if 'æ–™è™Ÿ' in df.columns:
    #     freq_map = df_train['æ–™è™Ÿ'].value_counts()
    #     df_train['æ–™è™Ÿ_FreqEnc'] = df_train['æ–™è™Ÿ'].map(freq_map).fillna(0).astype(int)
    #     df_test['æ–™è™Ÿ_FreqEnc']  = df_test['æ–™è™Ÿ'].map(freq_map).fillna(0).astype(int)
    #     features = features + ['æ–™è™Ÿ_FreqEnc']

    # exclude_cols = categorical_cols_for_ordinal + ['æ–™è™Ÿ_FreqEnc']
    # iqr_clip_feature = [f for f in features if f not in exclude_cols and "_Code" not in f]

    # print(iqr_clip_feature)
    
    # for c in iqr_clip_feature:
    #     try:
    #         df[c] = pd.to_numeric(df[c], errors="coerce")
    #         df[c] = iqr_clip(df[c])
    #     except Exception:
    #         pass
    
    # exclude_cols = ['']

    # iqr_clip_feature = [f for f in base_features if f not in exclude_cols and "_Code" not in f]

    # === éæ¿¾æ‰æ™‚é–“è¡ç”Ÿç‰¹å¾µï¼Œåªç•™çµ¦ preprocessor ç”¨çš„æ¬„ä½ ===
    features_for_preproc = [f for f in features if f not in time_features]

    # === å‚³çµ¦ build_preprocessor çš„å°±æ˜¯æ’é™¤å¾Œçš„ç‰ˆæœ¬ ===
    preprocessor, cat_cols, num_cols = build_preprocessor(df, features_for_preproc , time_features)
    # print(features_for_preproc)
    # print(time_features)
    # exit()

    all_features = features_for_preproc + time_features


    X_train = df_train[all_features]
    y_train = df_train[label_col].values.ravel()
    X_test  = df_test[all_features]
    y_test  = df_test[label_col].values.ravel()


    #preprocessor, cat_cols, num_cols = build_preprocessor(df, features)

    inner_cv = KFold(n_splits=INNER_FOLDS, shuffle=True, random_state=RANDOM_STATE)

    # ==== LightGBM æ¨¡å‹ ====
    # lgb = LGBMRegressor(
    #     objective="regression",
    #     random_state=RANDOM_STATE,
    #     n_jobs=-1,
    #     verbose=-1,
    #     force_col_wise=True,   # âœ… å° ColumnTransformer è¼¸å‡ºæœ€ç›¸å®¹
    # )

    # # ==== Pipeline ====
    # pipe = Pipeline(steps=[
    #     ("preprocess", preprocessor),  # ä¿ç•™ä½ çš„å‰è™•ç†æµç¨‹
    #     ("model", lgb)
    # ])

    # # ==== RandomizedSearchCV ====
    # search = RandomizedSearchCV(
    #     estimator=pipe,
    #     param_distributions=get_param_dist_lgb(),
    #     n_iter=N_ITER,
    #     cv=inner_cv,
    #     scoring="neg_mean_absolute_error",
    #     verbose=1,
    #     n_jobs=-1,
    #     random_state=RANDOM_STATE,
    #     refit=True,
    # )


    # æ¸¬è©¦ ColumnTransformer è¼¸å‡ºæ¬„ä½
    X_train_pre = preprocessor.fit_transform(X_train)
    print("âœ… ColumnTransformer è¼¸å‡º shape:", X_train_pre.shape)
    print("âœ… æ¬„ä½åç¨±å‰10:", X_train_pre.columns.tolist())

    # exit()
    # exit()

    # ==== è¨“ç·´ ====
    # lgb_model = search.fit(X_train, y_train)

    # # ==== Decision Tree æ¨¡å‹ ====
    # dt = DecisionTreeRegressor(
    #     random_state=RANDOM_STATE,
    # )

    # # ==== Pipeline ====
    # pipe = Pipeline(steps=[
    #     ("preprocess", preprocessor),  # å‰è™•ç†ï¼ˆç¸®æ”¾ã€ç·¨ç¢¼ç­‰ï¼‰
    #     ("model", dt)
    # ])

    # # ==== RandomizedSearchCV ====
    # search = RandomizedSearchCV(
    #     estimator=pipe,
    #     param_distributions=get_param_dist_dt(),
    #     n_iter=N_ITER,
    #     cv=inner_cv,
    #     scoring="neg_mean_absolute_error",   # å¯æ”¹ r2 / neg_root_mean_squared_error
    #     verbose=1,
    #     n_jobs=-1,
    #     random_state=RANDOM_STATE,
    #     refit=True,
    # )

    # # ==== è¨“ç·´ ====
    # dt_model = search.fit(X_train, y_train)


    # # ==== XGBoost æ¨¡å‹ ====
    # xgb = XGBRegressor(
    #     objective="reg:squarederror",   # è¿´æ­¸ä»»å‹™
    #     random_state=RANDOM_STATE,
    #     tree_method="gpu_hist"       # æœ‰ GPU æ”¹ "gpu_hist"
        
    # )

    # # ==== Pipeline ====
    # pipe = Pipeline(steps=[
    #     ("preprocess", preprocessor),   # å‰è™•ç†ï¼ˆä¾‹å¦‚ç¸®æ”¾ã€ç·¨ç¢¼ç­‰ï¼‰
    #     ("model", xgb)
    # ])

    # # ==== RandomizedSearchCV ====
    # search = RandomizedSearchCV(
    #     estimator=pipe,
    #     param_distributions=get_param_dist_xgb(),
    #     n_iter=N_ITER,
    #     cv=inner_cv,
    #     scoring="neg_mean_absolute_error",  # å¯æ”¹ç‚º r2 / neg_root_mean_squared_error
    #     verbose=1,
    #     n_jobs=-1,
    #     random_state=RANDOM_STATE,
    #     refit=True,   # ä»¥æœ€ä½³åƒæ•¸é‡è¨“
    # )

    # # # ==== è¨“ç·´ ====
    # xgb_model = search.fit(X_train, y_train)

    # 5) å…§å±¤ RandomizedSearchCV + KFoldï¼ˆåœ¨è¨“ç·´ fold ä¸Šå°‹åƒï¼‰
    rf = RandomForestRegressor(random_state=RANDOM_STATE)

    pipe = Pipeline(steps=[("preprocess", preprocessor), ("model", rf)])

    search = RandomizedSearchCV(
        estimator=pipe,
        param_distributions=get_param_dist(), #è¦æœå°‹çš„ã€Œè¶…åƒæ•¸ç©ºé–“ï¼ˆparameter spaceï¼‰ã€
        n_iter=N_ITER, # çµ„åˆæ•¸é‡
        cv=inner_cv,  # äº¤å‰é©—è­‰çš„è¨­å®š
        scoring="neg_mean_absolute_error", # æ¨¡å‹è©•åˆ†æŒ‡æ¨™
        verbose=1, # è¨“ç·´éç¨‹çš„è©³ç´°ç¨‹åº¦
        n_jobs=-1, # CPU å¹³è¡ŒåŒ–æ•¸é‡ï¼Œ-1æŒ‡å…¨CPU
        random_state=RANDOM_STATE, # å›ºå®šéš¨æ©Ÿæ€§ä¾†æºï¼Œç¢ºä¿å¯¦é©—å¯é‡ç¾
        refit=True,  # ä»¥æœ€ä½³åƒæ•¸åœ¨æ•´å€‹è¨“ç·´ fold é‡è¨“
    )
    rf_model = search.fit(X_train, y_train)

    print(f"[Fold {fold_idx}] Best params:", search.best_params_)
    print(f"[Fold {fold_idx}] Best MAE (cv, neg): {search.best_score_:.6f} | Best MAE: {-search.best_score_:.6f}")

    best_model = search.best_estimator_
    rf_model = best_model.named_steps["model"]     # å·² fit çš„ RandomForestRegressor
    preprocessor = best_model.named_steps["preprocess"]   # å·² fit
    #preprocessor = best_model.named_steps["preprocess"]

    # 6) å¤–å±¤æ¸¬è©¦é›†è©•ä¼°
    y_pred = best_model.predict(X_test)
    mae  = mean_absolute_error(y_test, y_pred)
    mse  = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2   = r2_score(y_test, y_pred)

    fold_metrics.append({
        "fold": fold_idx,
        "MAE":  mae,
        "MSE":  mse,
        "RMSE": rmse,
        "R2":   r2,
        "best_params": search.best_params_,
        "best_model": search.best_estimator_
    })
    print(f"[Fold {fold_idx}] MAE={mae:.4f}  MSE={mse:.4f}  RMSE={rmse:.4f}  RÂ²={r2:.4f}")

    # 7) ç‰¹å¾µé‡è¦åº¦ï¼ˆå¯é¸ï¼‰
    if hasattr(best_model, "feature_importances_"):
        for f, imp in zip(features, best_model.feature_importances_):
            fi_collector[f].append(imp)

    # ---- 9.1 æº–å‚™è³‡æ–™ï¼ˆå–ä¸€å€‹ä»£è¡¨æ€§çš„æ¨£æœ¬ï¼Œé¿å…å…¨é‡å¤ªæ…¢ï¼‰ ----------------------
    # SHAP ç¹ªåœ–å°æ¨£æœ¬æ•¸è¼ƒæ•æ„Ÿï¼Œæ•¸è¬ç­†ä»¥ä¸Šå¯æŠ½æ¨£ 2000~5000 ç­†å³å¯å‘ˆç¾è¶¨å‹¢
    #rf_model = pipe.named_steps["model"] 
    #xgb_model = pipe.named_steps["model"] 
    #############################################################
    # æŠ½æ¨£ï¼ˆé¿å…éæ…¢ï¼‰
    # def sample_frame(X, n=3000, random_state=42):
    #     return X if len(X) <= n else X.sample(n=n, random_state=random_state)

    # X_shap_raw = sample_frame(X_test, n=3000, random_state=42)

    # # â˜… å¿…é ˆï¼šç”¨å·² fit çš„å‰è™•ç†å™¨æŠŠåŸå§‹ç‰¹å¾µè½‰æˆæ•¸å€¼çŸ©é™£
    # X_shap_tx = preprocessor.transform(X_shap_raw)
    # if hasattr(X_shap_tx, "toarray"):   # sparse â†’ dense
    #     X_shap_tx = X_shap_tx.toarray()

    # # å–æœ€çµ‚ç‰¹å¾µåç¨±ï¼ˆå° OneHot éå¸¸é‡è¦ï¼‰
    # try:
    #     feat_names_out = preprocessor.get_feature_names_out()
    # except Exception:
    #     feat_names_out = np.array([f"f{i}" for i in range(X_shap_tx.shape[1])])

    # # ---- 9.2 è¨ˆç®— SHAP å€¼ -------------------------------------------------------
    # # å°æ–¼æ¨¹æ¨¡å‹ï¼Œä½¿ç”¨ TreeExplainerï¼ˆæ•ˆèƒ½/ç›¸å®¹æ€§è¼ƒä½³ï¼‰
    # explainer = shap.TreeExplainer(lgb_model)
    # #shap_values = explainer.shap_values(X_shap_tx, check_additivity=False)   # regression: (n_samples, n_features)
    # shap_values = explainer.shap_values(X_shap_tx)   # regression: (n_samples, n_features)
    # # åŸºæº–å€¼ï¼ˆæœŸæœ›è¼¸å‡ºï¼‰
    # expected_value = explainer.expected_value

    # X_shap_df = pd.DataFrame(X_shap_tx, columns=feat_names_out)

    # # ---- 9.3 åŒ¯å‡º mean(|SHAP|) ä½œç‚ºç‰¹å¾µå½±éŸ¿åŠ›æŒ‡æ¨™ -------------------------------
    # mean_abs_shap = np.abs(shap_values).mean(axis=0)
    # shap_importance_df = (
    #     pd.DataFrame({"feature": feat_names_out, "mean_abs_shap": mean_abs_shap})
    #     .sort_values("mean_abs_shap", ascending=False)
    # )
    # print("\n[SHAP] Top 20ï¼ˆmean |SHAP|ï¼‰")
    # print(shap_importance_df.head(20))

    # # ======================================================
    # # âœ… æ–°å¢ï¼šGroup OneHot â†’ åŸå§‹ç‰¹å¾µå±¤ç´š
    # # ======================================================
    # import re
    # from collections import defaultdict

    # def base_name_without_prefix(feat: str) -> str:
    #     """å»æ‰ ColumnTransformer çš„å‰ç¶´ï¼ˆä¾‹å¦‚ 'cat__', 'num__'ï¼‰"""
    #     return re.sub(r'^[^_]+__', '', feat)  # åˆªæ‰æœ€å‰é¢çš„ '<prefix>__'

    # def map_to_original_feature(feat: str, cat_cols, num_cols) -> str:
    #     """
    #     å°‡ One-Hot å¾Œæ¬„ä½å°æ‡‰å›åŸå§‹æ¬„ä½ï¼š
    #     - é¡åˆ¥ï¼šcat__æ–™è™Ÿ_A / cat__æ–™è™Ÿ[A] â†’ æ–™è™Ÿ
    #     - æ•¸å€¼ï¼šnum__æ•¸é‡ â†’ æ•¸é‡
    #     """
    #     name = base_name_without_prefix(feat)

    #     # å…ˆæª¢æŸ¥æ˜¯å¦ç‚ºæ•¸å€¼æ¬„ä½ï¼ˆå®Œå…¨ç›¸ç­‰å³å¯ï¼‰
    #     for c in num_cols:
    #         if name == c:
    #             return c

    #     # å†æª¢æŸ¥æ˜¯å¦ç‚ºé¡åˆ¥æ¬„ä½ï¼ˆåç¨±ä»¥ã€ŒåŸæ¬„ä½å_ã€æˆ–ã€ŒåŸæ¬„ä½å[ã€é–‹é ­ï¼‰
    #     for c in cat_cols:
    #         if name.startswith(c + "_") or name.startswith(c + "["):
    #             return c

    #     # è¬ä¸€å…©è€…éƒ½æ²’å‘½ä¸­ï¼Œå°±å˜—è©¦æ›´ä¿å®ˆåœ°é‚„åŸï¼šå»æ‰é¡åˆ¥å€¼å°¾å·´ï¼ˆ_xxx æˆ– [xxx]ï¼‰
    #     name2 = re.sub(r'\[.*\]$', '', name)   # å»æ‰ [xxx]
    #     name2 = re.sub(r'_(?!.*_).*$', '', name2)  # å»æ‰æœ€å¾Œä¸€å€‹ '_' å¾Œçš„å­—
    #     return name2

    # # ç”¢ç”Ÿã€ŒOne-Hot è¼¸å‡ºæ¬„ä½ã€â†’ã€ŒåŸå§‹æ¬„ä½ã€çš„å°æ‡‰è¡¨
    # feat_group_map = {}
    # for f in feat_names_out:
    #     feat_group_map[f] = map_to_original_feature(f, cat_cols=cat_cols, num_cols=num_cols)

    # # ä¾åŸå§‹æ¬„ä½å½™ç¸½ mean(|SHAP|)
    # grouped_importance = defaultdict(float)
    # for f_name, shap_val in zip(feat_names_out, mean_abs_shap):
    #     orig_name = feat_group_map.get(f_name, f_name)
    #     grouped_importance[orig_name] += shap_val  # ä¹Ÿå¯æ”¹æˆ .mean()ï¼›æ­¤è™•ç”¨åŠ ç¸½ä»£è¡¨æ•´é«”å½±éŸ¿åŠ›

    # grouped_shap_importance_df = (
    #     pd.DataFrame(list(grouped_importance.items()), columns=["feature", "mean_abs_shap"])
    #     .sort_values("mean_abs_shap", ascending=False)
    #     .reset_index(drop=True)
    # )

    # print("\n[SHAP] Top 20ï¼ˆmean |SHAP|ï¼Œåˆä½µå›åŸå§‹æ¬„ä½ï¼‰")
    # print(grouped_shap_importance_df)

    # # è¦–è¦ºåŒ–ï¼ˆåˆ—å‡ºå„å€‹åŸå§‹ç‰¹å¾µï¼‰
    # #topN = 20
    # plt.figure(figsize=(10, 0.4 * len(grouped_shap_importance_df)))  # æ ¹æ“šç‰¹å¾µæ•¸é‡è‡ªå‹•èª¿æ•´é«˜åº¦
    # plt.barh(
    #     grouped_shap_importance_df["feature"][::-1],         # æ‰€æœ‰ç‰¹å¾µï¼ˆåè½‰è®“æœ€é‡è¦çš„åœ¨æœ€ä¸Šé¢ï¼‰
    #     grouped_shap_importance_df["mean_abs_shap"][::-1]
    # )
    # plt.xlabel("Mean |SHAP value|")
    # plt.title("Grouped SHAP Feature Importance (All Features)")
    # plt.tight_layout()
    # plt.show()

    # x = np.arange(len(y_test))

    # plt.figure(figsize=(12, 6))
    # plt.plot(x, y_pred, label="True (y_test)", color="blue", linewidth=2)
    # plt.plot(x, y_test, label="Predicted (y_pred)", color="red", linestyle="--", linewidth=2)

    # plt.title("Prediction vs True Value", fontsize=16)
    # plt.xlabel("Sample Index", fontsize=14)
    # plt.ylabel("Value", fontsize=14)
    # plt.legend()
    # plt.grid(True)
    # plt.show()
    #############################################################


# =========================
# çµæœå½™æ•´
# =========================
metrics_df = pd.DataFrame(fold_metrics)
print("\n=== K-Fold æˆç¸¾ï¼ˆé€æŠ˜ï¼‰ ===")
print(metrics_df)

print("\n=== K-Fold å¹³å‡æˆç¸¾ ===")
print(metrics_df[["MAE", "MSE", "RMSE", "R2"]].mean())

# è·¨æŠ˜å¹³å‡ç‰¹å¾µé‡è¦åº¦
if len(fi_collector) > 0:
    fi_avg = pd.DataFrame({
        "feature": list(fi_collector.keys()),
        "importance_mean": [np.mean(vals) for vals in fi_collector.values()],
        "importance_std":  [np.std(vals)  for vals in fi_collector.values()],
    }).sort_values("importance_mean", ascending=False)
    print("\n=== å¹³å‡ç‰¹å¾µé‡è¦åº¦ï¼ˆè·¨æŠ˜ï¼‰ ===")
    print(fi_avg.to_string(index=False))


### æœ€ä½³åŒ–åƒæ•¸å„²å­˜ï¼Œä¸¦é‡è¨“å¾Œå°‡æ¨¡å‹å„²å­˜

# from collections import Counter
# import joblib

# # çµ±è¨ˆæ¯æ¬¡ fold çš„æœ€ä½³åƒæ•¸
# param_counter = Counter([tuple(sorted(d["best_params"].items())) for d in fold_metrics])
# final_params = dict(param_counter.most_common(1)[0][0])  # å‡ºç¾æ¬¡æ•¸æœ€å¤šçš„çµ„åˆ

# # âœ… ç§»é™¤å‰ç¶´ "model__"
# model_params = {k.replace("model__", ""): v for k, v in final_params.items() if k.startswith("model__")}


# # é‡å»º Pipelineï¼ˆç”¨åŒæ¨£çš„ preprocessorï¼‰
# final_model = Pipeline(steps=[
#     ("preprocess", preprocessor),  # æˆ–é‡å»ºæ–°çš„ fit_transform
#     ("model", RandomForestRegressor(random_state=42, **model_params))
# ])

# train_len = int(len(df)*0.8)
# model_train = df[:train_len]
# model_test = df[train_len:]

# final_model.fit(model_train[all_features], model_train[label_col])
# joblib.dump(final_model, "models/final_rf_without_part_num_divide_v2.pkl")

# print("âœ… Final model retrained with all data and best params.")
